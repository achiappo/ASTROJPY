{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recall the log-Likelihood formula (with the $2\\pi$ term removed):\n",
    "\\begin{equation}\n",
    "{\\cal L} = -\\log L = \\frac{1}{2}\\sum_{i=1}^N\\left[\\log(\\sigma^2+\\varepsilon_i^2)+\\frac{(v_i-u)^2}{\\sigma^2+ \\varepsilon_i^2}\\right]\n",
    "\\end{equation}\n",
    "Let's consider that $\\sigma$ and $u$ are the parameters to be fitted (following the work by http://arxiv.org/abs/0904.3329). Partial derivation of $\\cal L$ with respect to these yields :\n",
    "\\begin{align}\n",
    "\\sum_{i=1}^N \\frac{v_i}{\\sigma^2+\\varepsilon_i^2} &= u\\sum_{i=1}^N \\frac{1}{\\sigma^2+\\varepsilon_i^2}\\\\\n",
    "\\sum_{i=1}^N \\frac{(v_i-u)^2}{(\\sigma^2+\\varepsilon_i^2)^2} &= \\sum_{i=1}^N \\frac{1}{\\sigma^2+\\varepsilon_i^2}\n",
    "\\end{align} \n",
    "Note that in our code, we have been using $u=u_0\\equiv\\sum_i^N v_i/N$, the sample average velocity. One can check on the first equation above that this is the solution for $u$ in the limit of small velocity measurement errors, compared to the dispersion $\\sigma$ : $(\\varepsilon_i/\\sigma)^2\\rightarrow 0\\quad \\forall i$. In such limit, the second equation yields $\\sigma = \\sigma_0\\equiv\\sum_i^N (v_i-u_0)^2/N$, the sample variance.\n",
    "\n",
    "Thus in the limit of vanishing measurement errors, the gaussian likelihood has a trivial solution : $\\sigma(\\rho_0, r_0) = \\sigma_0$, which defines the minimal curve on the $(\\rho_0,\\,r_0)$ plane of our DM density parameters. Thus, we should first of all check whether $\\mathrm{max}_i\\varepsilon_i\\ll\\sigma_0$ for our dwarfs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ursa Major 1 : conservative 3.3286 likely more correct 0.659221\n",
      "Sculptor : conservative 1.28245 likely more correct 0.273852\n",
      "Fornax : conservative 1.09073 likely more correct 0.165878\n",
      "Bootes 1 : conservative 2.96312 likely more correct 0.831952\n",
      "Draco : conservative 0.99117 likely more correct 0.36275\n",
      "Coma Berenices : conservative 3.27191 likely more correct 0.976253\n",
      "Sagittarius : conservative 0.313037 likely more correct 0.146962\n",
      "Segue 1 : conservative 5.36808 likely more correct 0.924557\n",
      "Willman 1 : conservative 1.54722 likely more correct 0.808797\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "names = {'dra':\"Draco\",'seg1':\"Segue 1\",'umaI':\"Ursa Major 1\",'booI':\"Bootes 1\",\n",
    "         'wil1':\"Willman 1\",'scl':\"Sculptor\",'for':\"Fornax\",'sgr':\"Sagittarius\",\n",
    "         'com':\"Coma Berenices\"}\n",
    "for gal in names :\n",
    "    x,v,dv = np.loadtxt('data/velocities/velocities_'+gal+'.dat',dtype=float,usecols=(0,1,2),unpack=True)\n",
    "    u_0 = v.mean()\n",
    "    s_0 = v.std()\n",
    "    print \"%s : conservative %g likely more correct %g\"%(names[gal],dv.max()/s_0,dv.mean()/s_0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So taking the max of the error shows that we are not completely safe in neglecting the measurement errors, but taking their average is perhaps a better indication? Nevertheless, we probably want to try to solve for $\\sigma$ and $u$ with the measurement errors. This can probably be done iteratively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ursa Major 1 : -0.00562636\n",
      "Sculptor : 0.000550519\n",
      "Fornax : 4.53144e-05\n",
      "Bootes 1 : 0.00255312\n",
      "Draco : -0.000237632\n",
      "Coma Berenices : 0.00345137\n",
      "Sagittarius : 7.35223e-05\n",
      "Segue 1 : 0.00277647\n",
      "Willman 1 : -0.0392069\n"
     ]
    }
   ],
   "source": [
    "def g(s,v,dv):\n",
    "    d = s*s + dv*dv\n",
    "    u = (v/d).sum()/(1./d).sum()\n",
    "    return u\n",
    "\n",
    "for gal in names:\n",
    "    x,v,dv = np.loadtxt('data/velocities/velocities_'+gal+'.dat',dtype=float,usecols=(0,1,2),unpack=True)\n",
    "    u_0 = v.mean()\n",
    "    s_0 = v.std()\n",
    "    u_1 = g(s_0, v, dv)\n",
    "    print \"%s : %g\"%(names[gal],abs(u_1 - u_0)/u_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The very small relative change between $u_0=g(\\sigma_0,v,0)$ and $u_1 = g(\\sigma_0,v,dv)$ is a very good sign that the initial guess with neglected errors is close to the correct solution, and thus that neglecting errors is probably not grossly inaccurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ursa Major 1 7.81108018788 7.56733020435\n",
      "Sculptor 9.43504091239 8.95353536576\n",
      "Fornax 10.9101270609 10.6949157573\n",
      "Bootes 1 4.38727287336 \n",
      "Bootes 1  failed\n",
      "Bootes 1 4.38727287336 4.39433747662\n",
      "Draco 9.27187479585 8.54081086709\n",
      "Coma Berenices 6.57720745563 4.63235383369\n",
      "Sagittarius 15.9405941804 15.7258738841\n",
      "Segue 1 7.04907415167 \n",
      "Segue 1  failed\n",
      "Segue 1 7.04907415167 7.07305317348\n",
      "Willman 1 5.93968230434 4.82532860248\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import newton\n",
    "def h(v, dv, prev_s, curr_u):\n",
    "    eq = lambda s : ( ((v-curr_u)/(s*s+dv*dv))**2 - (1./(s*s+dv*dv)) ).sum()\n",
    "    new_s = newton(eq, prev_s)\n",
    "    return new_s\n",
    "\n",
    "for gal in names:\n",
    "    x,v,dv = np.loadtxt('data/velocities/velocities_'+gal+'.dat',dtype=float,usecols=(0,1,2),unpack=True)\n",
    "    s_0 = v.std()\n",
    "    u_1 = g(s_0, v, dv)\n",
    "    try :\n",
    "        print names[gal], s_0, h(v,dv, s_0, u_1)\n",
    "    except RuntimeError:\n",
    "        print '\\n',names[gal], ' failed'\n",
    "        print names[gal], s_0, h(v,np.zeros_like(v), s_0, u_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it seems that Segue 1 and Bootes 1 are causing trouble here, and the stepping may have to compute the next $\\sigma$ based on neglected errors and the updated $u$. Need to investigate what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we can call it converged!  209.010329694 7.07293029247 209.590757576 7.04907415167\n"
     ]
    }
   ],
   "source": [
    "def update(prev_u, prev_s):\n",
    "    new_u = g(prev_s, v, dv)\n",
    "    try :\n",
    "        new_s = h(v, dv, prev_s, new_u)\n",
    "    except :\n",
    "        new_s = h(v, np.zeros_like(v), prev_s, new_u)\n",
    "    return np.max([abs(new_u-prev_u), abs(new_s-prev_s)]), new_u, new_s\n",
    "\n",
    "#full draco :\n",
    "x,v,dv = np.loadtxt('data/velocities/velocities_seg1.dat',dtype=float,usecols=(0,1,2),unpack=True)\n",
    "u = v.mean()\n",
    "s = v.std()\n",
    "conv = False\n",
    "while not conv :\n",
    "    eps, u, s = update(u, s)\n",
    "    if eps <1.e-5 :\n",
    "        conv = True\n",
    "print \"we can call it converged! \", u, s, v.mean(), v.std()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
